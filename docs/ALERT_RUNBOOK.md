# アラート対応手順書（Runbook）

AIフィットネスアプリのアラート発生時の対応手順です。

## 1. 対応フロー概要

```
アラート発生
    │
    ▼
┌─────────────────────┐
│ 1. アラート確認     │
│    - 内容把握       │
│    - 影響範囲確認   │
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│ 2. 初期対応         │
│    - 必要に応じて   │
│      エスカレーション│
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│ 3. 調査・診断       │
│    - ログ確認       │
│    - メトリクス確認 │
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│ 4. 対処             │
│    - 修正/緩和      │
│    - 検証           │
└─────────┬───────────┘
          │
          ▼
┌─────────────────────┐
│ 5. クローズ         │
│    - 報告書作成     │
│    - 再発防止策     │
└─────────────────────┘
```

## 2. アラート別対応手順

### 2.1 API Error Rate High（エラー率上昇）

**条件**: エラー率 > 1%（5分間）

**重要度**: High

**対応時間目標**: 1時間以内

#### 初期確認

1. Cloud Monitoring で現在のエラー率を確認
2. Error Reporting で発生しているエラーの種類を特定
3. 影響を受けているエンドポイントを特定

#### 調査手順

```
# Cloud Logging クエリ
resource.type="cloud_function"
severity>=ERROR
timestamp>="[アラート発生時刻-10分]"
```

1. エラーメッセージとスタックトレースを確認
2. 特定のユーザーまたはリクエストパターンに集中しているか確認
3. 最近のデプロイとの関連を確認

#### 対処方法

| 原因 | 対処 |
|------|------|
| コードバグ | 前バージョンにロールバック |
| 外部サービス障害 | サーキットブレーカー発動/フォールバック |
| リソース枯渇 | インスタンス数/メモリ増加 |
| 入力データ異常 | バリデーション強化 |

#### 検証

- エラー率が正常範囲（< 0.5%）に戻ったことを確認
- 影響を受けたユーザーがいる場合は通知を検討

---

### 2.2 API Latency High（レイテンシ上昇）

**条件**: P99 レイテンシ > 5秒（5分間）

**重要度**: Medium

**対応時間目標**: 4時間以内

#### 初期確認

1. Cloud Monitoring でレイテンシグラフを確認
2. 遅延しているエンドポイントを特定
3. トラフィック量の変化を確認

#### 調査手順

```
# Cloud Logging クエリ - 遅いリクエスト
resource.type="cloud_function"
jsonPayload.durationMs>=5000
timestamp>="[アラート発生時刻-10分]"
```

1. Cloud Trace で遅いトレースを分析
2. Firestore クエリのパフォーマンスを確認
3. 外部 API 呼び出しの遅延を確認

#### 対処方法

| 原因 | 対処 |
|------|------|
| コールドスタート | min_instances 設定 |
| Firestore クエリ | インデックス追加/クエリ最適化 |
| 外部 API 遅延 | タイムアウト設定/キャッシュ導入 |
| 高負荷 | スケールアウト |

---

### 2.3 Auth Failures Spike（認証失敗急増）

**条件**: 認証失敗 > 10回/5分

**重要度**: High

**対応時間目標**: 1時間以内

#### 初期確認

1. セキュリティダッシュボードを確認
2. 失敗の発生元 IP アドレスを確認
3. 特定のユーザーに集中しているか確認

#### 調査手順

```
# Cloud Logging クエリ - 認証失敗
resource.type="cloud_function"
jsonPayload.eventType="LOGIN_FAILURE"
timestamp>="[アラート発生時刻-10分]"
```

```
# IP アドレス別の失敗数
resource.type="cloud_function"
jsonPayload.eventType="LOGIN_FAILURE"
```

1. 同一 IP からの連続失敗を確認（ブルートフォース）
2. 複数アカウントへの試行を確認（クレデンシャルスタッフィング）
3. 正規ユーザーの影響を確認

#### 対処方法

| 原因 | 対処 |
|------|------|
| ブルートフォース | IP ブロック + アカウントロック |
| ボット攻撃 | reCAPTCHA 強化 |
| パスワードリセット問題 | ユーザー通知 |
| システム障害 | 認証サービス確認 |

---

### 2.4 Firestore Error Rate High（Firestore エラー）

**条件**: Firestore エラー > 5回/5分

**重要度**: High

**対応時間目標**: 1時間以内

#### 初期確認

1. Firebase Console で Firestore ステータスを確認
2. エラーの種類を特定（権限、クォータ、接続）

#### 調査手順

```
# Cloud Logging クエリ
resource.type="cloud_function"
severity>=ERROR
jsonPayload.message:"Firestore"
```

1. セキュリティルールエラーの確認
2. クォータ制限の確認
3. インデックスエラーの確認

#### 対処方法

| 原因 | 対処 |
|------|------|
| セキュリティルール | ルール修正/デプロイ |
| クォータ超過 | クォータ増加申請 |
| インデックス不足 | インデックス追加 |
| 接続エラー | リトライロジック確認 |

---

### 2.5 Budget Alert（予算アラート）

**条件**: 月間予算の 50%/80%/90%/100% 到達

**重要度**: Low → High（閾値に応じて）

#### 対応マトリクス

| 閾値 | 対応 |
|------|------|
| 50% | 傾向確認、月末予測 |
| 80% | コスト分析、削減計画 |
| 90% | 緊急削減実施 |
| 100% | サービス制限検討 |

#### コスト分析手順

1. Cloud Billing レポートを確認
2. サービス別コスト内訳を確認
3. 異常な増加のあるサービスを特定

#### 削減オプション

- Cloud Functions: maxInstances 削減
- Firestore: 不要なインデックス削除
- BigQuery: クエリ最適化
- Cloud Storage: ライフサイクルルール適用

---

### 2.6 Uptime Check Failed（ヘルスチェック失敗）

**条件**: 2回連続の失敗

**重要度**: Critical

**対応時間目標**: 15分以内

#### 初期確認

1. サービスの実際の状態を確認
2. ステータスページを確認
3. 他のモニタリングシステムの状態を確認

#### 調査手順

1. Cloud Functions のログを確認
2. Firebase Console でサービス状態を確認
3. ネットワーク接続性を確認

#### 対処方法

| 原因 | 対処 |
|------|------|
| デプロイ失敗 | 前バージョンにロールバック |
| リソース枯渇 | 再起動/スケールアップ |
| Firebase 障害 | ステータスページ確認/サポート連絡 |
| ネットワーク障害 | Google Cloud ステータス確認 |

---

## 3. セキュリティインシデント対応

### 3.1 ブルートフォース攻撃検知

**条件**: 5回以上の連続ログイン失敗

#### 即時対応

1. 該当 IP アドレスを WAF/ファイアウォールでブロック
2. 影響を受けたアカウントをロック
3. セキュリティチームに通知

#### 調査

```
# 攻撃元 IP の全アクティビティ
resource.type="cloud_function"
jsonPayload.sourceIp="[攻撃元IP]"
```

#### 事後対応

- ユーザーへのパスワードリセット案内
- セキュリティイベントの記録
- 再発防止策の検討

### 3.2 大量データダウンロード検知

**条件**: 100件以上のレコード/分

#### 即時対応

1. 該当セッションを強制終了
2. ユーザーアカウントの一時停止
3. セキュリティチームに通知

#### 調査

- データの種類と範囲を特定
- 意図的な行為か誤操作かを判断
- データ漏洩の可能性を評価

### 3.3 GDPR 関連インシデント

**条件**: 個人データの不正アクセス/漏洩の可能性

#### 即時対応（72時間ルール）

1. インシデントを記録
2. 法務/DPO に通知
3. 影響範囲の特定を開始

#### 報告義務

- 72時間以内に監督当局への報告（重大な場合）
- 影響を受けたユーザーへの通知

---

## 4. エスカレーションマトリクス

| 重要度 | 1次対応 | エスカレーション先 | 時間 |
|--------|--------|------------------|------|
| Critical | オンコール | 管理者 + 全チーム | 即時 |
| High | オンコール | チームリード | 30分後 |
| Medium | 担当者 | チームリード | 2時間後 |
| Low | 担当者 | - | 翌営業日 |

## 5. 連絡先

| 役割 | 連絡先 |
|------|--------|
| オンコール | #ai-fitness-alerts (Slack) |
| セキュリティ | #ai-fitness-security (Slack) |
| 管理者 | ops@ai-fitness.example.com |

## 6. 事後報告テンプレート

```markdown
# インシデント報告

## 概要
- 日時:
- 重要度:
- 影響範囲:

## タイムライン
- [時刻] アラート発生
- [時刻] 調査開始
- [時刻] 原因特定
- [時刻] 対処完了
- [時刻] 正常確認

## 根本原因
[詳細を記載]

## 対処内容
[実施した対処を記載]

## 再発防止策
[検討した防止策を記載]

## 教訓
[得られた教訓を記載]
```
